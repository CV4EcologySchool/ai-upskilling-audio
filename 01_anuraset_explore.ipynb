{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ac4666",
   "metadata": {},
   "source": [
    "# üê∏¬†AnuraSet Audio Lab 1\n",
    "\n",
    "**Data Exploration with Python**\n",
    "\n",
    "The first portion of this workshop involves getting familiar with *agile modeling*, a framework for efficiently training new machine learning classifiers. In this workshop, you play the role of a herpetologist who has returned from field work in Brazil with a large collection of audio recordings of frog species. You are hoping to use agile modeling to quickly tell you which audio files contain a species that is of interest to you.\n",
    "\n",
    "You have already used the agile modeling codebase to extract *embeddings* for a portion of this audio (which for this workshop is represented by the [AnuraSet](https://github.com/soundclim/anuraset) dataset). It now remains to pick your favorite species and train a new machine learning model to classify whether or not each embedding contains that species.\n",
    "\n",
    "Agile modeling is a *human-in-the-loop* approach to doing so. We assume all of your data is unlabeled: you have no idea what each audio file contains. Agile modeling uses *active learning* to: (1) decide which data points in your collection should be labeled, and (2) ask you to label them. Active learning analyzes the structure of the *embedding space* to determine which data points will provide the most information once you label them. For instance, if many vocalizations sound similar to one another, and therefore form a *cluster* in the embedding space, it is possible that labeling a single data point in that cluster will tell you the species label of *all* points in that cluster. Active learning exploits structure like this, repeatedly querying for the labels of highly-informative data points.\n",
    "\n",
    "In order to be an effective human in this loop, you'll need to know how to identify your favorite frog species by ear! In this notebook, you will train yourself to do so through *exploratory data analysis* of the AnuraSet dataset in Python. In doing so, you will practice some very common data exploration and analysis techniques that will assist you later in building your own classifier from scratch:\n",
    "\n",
    "* manipulate CSV data with **pandas**\n",
    "* visualize summaries with **matplotlib** and **seaborn**\n",
    "* load & play audio clips with **librosa** / **IPython.display.Audio**\n",
    "* plot spectrograms\n",
    "* write a small object‚Äëoriented wrapper for audio samples\n",
    "* identify a frog species of your choosing using your own human brain! (Next time we will teach the machine)\n",
    "\n",
    "We have provided an outline of this process below. Fill in the **üìù¬†Exercise** sections yourself.\n",
    "[NOTE: These are filled in with example solutions now; we can remove them before sending to students] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf797a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SETUP -¬†Imports\n",
    "# ------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio, display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import librosa\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5148b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# DATA LOCATION - Change if you are using a different machine\n",
    "# ------------------------------------------------------------\n",
    "DATA_DIR = Path('/mnt/class_data/anuraset')\n",
    "META_PATH = DATA_DIR / 'metadata.csv'\n",
    "AUDIO_DIR = DATA_DIR / 'audio'\n",
    "\n",
    "assert META_PATH.exists(), f\"{META_PATH} does not exist. Update DATA_DIR!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de22ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†0¬†‚Äì¬†Load & inspect metadata\n",
    "# ------------------------------------------------------------\n",
    "# For this notebook, we will stray a little bit from our story that you are a herpetologist with a boatload of\n",
    "# raw data. First we have to do some *human learning* (i.e., train you to identify frogs by ear). To do so, \n",
    "# we will take advantage of the data processing that has already been done by the creators of AnuraSet: namely\n",
    "# the splitting of raw audio into 3-second chunks with detailed metadata about what species are present in\n",
    "# each chunk. You will use these audio files to build a frog identification curriculum for yourself.\n",
    "#\n",
    "# The metadata is a CSV file located at META_PATH. \n",
    "# \n",
    "# 1. First, open it with pandas and take a look at the first few rows using the `head()` method. \n",
    "# Try to understand what each column might refer to; check out the AnuraSet paper \n",
    "# (https://www.nature.com/articles/s41597-023-02666-2) if anything is not clear.\n",
    "# ------------------------------------------------------------\n",
    "meta = None # TODO: Fill this in\n",
    "if meta is not None:\n",
    "    print(f\"Loaded {len(meta):,} rows x {meta.shape[1]} columns\")\n",
    "    meta.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37433403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Species indicator columns\n",
    "# ------------------------------------------------------------\n",
    "# In AnuraSet each species has its own binary column (1 = species vocalization\n",
    "# present in the 3‚Äësecond sample).  Let's grab just those columns.\n",
    "species_cols = meta.columns[8:]   # first 8 columns are meta‚Äëinfo\n",
    "print(f\"Detected {len(species_cols)} species columns\")\n",
    "species_cols[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†1¬†‚Äì¬†Count how many samples for each species\n",
    "# ------------------------------------------------------------\n",
    "# First, let's see how many example recordings of each species we are working with.\n",
    "# \n",
    "# 1. First, create a pandas¬†Series mapping species ‚Üí number¬†of¬†positive samples.\n",
    "# 2. Then, plot the counts of the top 10 most common species as a bar chart using matplotlib.\n",
    "# ------------------------------------------------------------\n",
    "species_counts = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c029d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†2¬†‚Äì¬†Inspect co-occurrance\n",
    "# ------------------------------------------------------------\n",
    "# Species that occur by themselves will likely be easier to learn to identify, because\n",
    "# there will be fewer vocalizations from other species at the same time. Let's see\n",
    "# which species are likely to be present alongside which other species.\n",
    "# \n",
    "# 1. For the top 10 most common species (which you found above), build a 'co-occurrance' matrix.\n",
    "#    This should be a 10x10 matrix where the value at cell (i,j) represents the number of clips\n",
    "#    that contain *both* species i and species j. (hint: look up pandas.Series.dot)\n",
    "#\n",
    "# 2. Visualize this matrix using matplotlib or seaborn. (hint: look up sns.heatmap). Which species\n",
    "#    are most likely to co-occur? Which are most likely to occur alone? Feel free to explore further\n",
    "#    by, e.g., normalizing across rows or columns to answer these questions.\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a0883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Choose a target species - You will learn to classify it yourself!\n",
    "# Pick a species code that interests you.¬†(You can always come back and choose \n",
    "# a different one later if your first choice is too dificult. \n",
    "# \n",
    "# Change TARGET below.\n",
    "# ------------------------------------------------------------\n",
    "TARGET = 'SPHSUR'  \n",
    "assert TARGET in species_cols, f\"{TARGET} not a valid column!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9296378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†3¬†‚Äì¬†Find recordings dominated by the target species.\n",
    "# ------------------------------------------------------------\n",
    "# Now let's find some audio clips that contain your TARGET species and not much else.\n",
    "# It will be easier to learn from these clips as you will be able to isolate the vocalizations\n",
    "# of the TARGET.\n",
    "#\n",
    "# 1. Filter your `meta` dataframe down to clips (i.e. rows) that satisfy two criteria: \n",
    "#       (i) they contain your TARGET species, and\n",
    "#       (ii) they contain 2 or fewer species total.\n",
    "# \n",
    "# 2. Store the filtered dataframe as `solo_df`. Print its length, and display its `head()`.\n",
    "# ------------------------------------------------------------\n",
    "solo_df = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†4 - Load & play audio\n",
    "# ------------------------------------------------------------\n",
    "# Now let's explore these recordings and see if you can learn to recognize your chosen species!\n",
    "#\n",
    "# 1. First, complete the filepath_from_row method. Convert the row of a dataframe to\n",
    "#    the filepath of the corresponding audio file. (hint: explore DATA_DIR either in\n",
    "#    python or on the command line to understand how the files are organized.)\n",
    "# \n",
    "# 2. Complete the play_audio method. Load the audio file from a given dataframe row\n",
    "#    with librosa.load and display it in this notebook with IPython.display.Audio.\n",
    "#    (hint: this will involve keeping track of a *sample rate*, i.e., the frequency at which\n",
    "#    we sample from a continuous audio signal in order to get a discrete representation that can\n",
    "#    be stored digitally. When you load the audio file, you can tell librosa.load to use the `native`\n",
    "#    sample rate of the audio file; when you play the audio file, you will want to match this. For fun,\n",
    "#    try doubling or halving the sample rate when you play it back; what happens?)\n",
    "#\n",
    "# 3. Select 10 random clips from solo_df and play their audio using the methods you just wrote.\n",
    "#    See if you can identify patterns in the clips and begin to learn how to identify your target species.\n",
    "# ------------------------------------------------------------\n",
    "def filepath_from_row(row, data_loc=str(DATA_DIR / 'audio')):\n",
    "    pass\n",
    "\n",
    "def play_audio(row):\n",
    "    pass\n",
    "\n",
    "# then, do something like:\n",
    "# print(\"Audio from\", TARGET)\n",
    "# for i in range(10):\n",
    "# random_idx = ...\n",
    "# play_audio(solo_df.iloc[random_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b0910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Human Learning exercise - Quiz yourself!\n",
    "# ------------------------------------------------------------\n",
    "# This applet will randomly select one audio file containing the TARGET\n",
    "# species, and one audio file that does not. See if you can get the classification\n",
    "# correct! Play this until you have successfully trained your biological neural \n",
    "# network to convergence.\n",
    "# ------------------------------------------------------------\n",
    "output = widgets.Output()\n",
    "\n",
    "def audio_quiz():\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Select one clip of the target species at random\n",
    "    target_row = meta[(meta[TARGET] == 1)].sample(1).iloc[0]\n",
    "    \n",
    "    # Select one clip of a non-target species at random\n",
    "    non_target_row = meta[(meta[TARGET] == 0)].sample(1).iloc[0]\n",
    "    \n",
    "    # Shuffle the two clips\n",
    "    clips = [target_row, non_target_row]\n",
    "    random.shuffle(clips)\n",
    "    \n",
    "    # Create quiz applet\n",
    "    print(\"Listen to the two audio clips below and select the target species!\")\n",
    "    print(\"Target species:\", TARGET)\n",
    "    for i, clip in enumerate(clips):\n",
    "        print(f\"\\nClip {i + 1}:\")\n",
    "        play_audio(clip)\n",
    "    \n",
    "    def on_button_click(button):\n",
    "        clear_output(wait=True)\n",
    "        if clips[int(button.description) - 1][TARGET] == 1:\n",
    "            next_button = widgets.Button(description=\"CORRECT!! Try again?\", layout=widgets.Layout(width='50%'))\n",
    "        else:\n",
    "            next_button = widgets.Button(description=\"WRONG!! Try again?\", layout=widgets.Layout(width='50%'))\n",
    "        \n",
    "        next_button.on_click(lambda _: audio_quiz())  # Reset the quiz\n",
    "        display(next_button)\n",
    "    \n",
    "    print(\"Which clip contains\", TARGET, \"?\")\n",
    "    button1 = widgets.Button(description=\"1\")\n",
    "    button2 = widgets.Button(description=\"2\")\n",
    "    button1.on_click(on_button_click)\n",
    "    button2.on_click(on_button_click)\n",
    "    display(widgets.HBox([button1, button2]))\n",
    "\n",
    "# Start the quiz\n",
    "display(output)\n",
    "audio_quiz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†5¬†‚Äì¬†Visualizing audio data: Waveforms\n",
    "# ------------------------------------------------------------\n",
    "# How are audio classification networks able to do what they do? A lot of this\n",
    "# has to do with the representation used for the audio itself when it is input\n",
    "# to the network. Let's explore some options here.\n",
    "#\n",
    "# The first way that audio is commonly represented is as a *waveform* which\n",
    "# shows amplitude vs. time. Let's see what this looks like.\n",
    "#\n",
    "# 1. Select an example audio file containing your TARGET species. Play the clip using\n",
    "#    your play_audio method from above. Choose a clip where your species is clearly audible.\n",
    "#\n",
    "# 2. Use librosa.display.waveshow to display the waveform of your audio. Use plt.xlabel\n",
    "#    and plt.ylabel to correctly label the axes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Visualizing audio data: Fourier transforms\n",
    "# ------------------------------------------------------------\n",
    "# Waveforms are useful for identifying transient events -- i.e. instantaneous\n",
    "# sharp changes in signal amplitude -- but they are not extremely useful for\n",
    "# classifying what is there. This is because we perceive audio *frequencies*, i.e,\n",
    "# modulations of amplitude at a certain rate over time. This is hard to see in\n",
    "# waveform diagrams.\n",
    "#\n",
    "# Let's try transforming our sample clip with a discrete Fourier Transform (DFT),\n",
    "# which decomposes an audio signal into the set of frequencies it contains.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# assumes your audio file, loaded from librosa, is stored as `audio`\n",
    "dft = np.fft.rfft(audio)\n",
    "\n",
    "# get the amplitude spectrum in decibels\n",
    "amplitude = np.abs(dft)\n",
    "amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)\n",
    "\n",
    "# get the frequency bins\n",
    "frequency = librosa.fft_frequencies(sr=sr, n_fft=len(audio))\n",
    "\n",
    "plt.plot(frequency, amplitude_db)\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Amplitude (dB)\")\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37de621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†6a¬†‚Äì¬†Visualizing audio data: Spectrograms\n",
    "# ------------------------------------------------------------\n",
    "# This is starting to look more interpretable -- we can see whether our file contains\n",
    "# more low-frequency or high-frequency content, which may point to what it contains.\n",
    "# But this is still a summary of the entire clip; it would be useful to see how frequency\n",
    "# content changes over time. What if we took a lot of DFTs, each over a short window of time,\n",
    "# and then arranged them sequentially to see how frequency content changes over time? A \n",
    "# *spectrogram* does exactly this.\n",
    "#\n",
    "# Spectrograms have contributed to much much of recent success in audio classification. \n",
    "# Perhaps surprisingly, this is also related to advancements in *computer vision*. As we\n",
    "# will see, the visual representation offered by a spectrogram is informative enough that\n",
    "# we can train visual classification networks with them. This allows us to use standard \n",
    "# computer vision classifiers rather than having to build custom neural network architectures \n",
    "# for audio.\n",
    "# \n",
    "# The pre-trained models we will use, YAMNet and Perch, perform the conversion from\n",
    "# audio to spectrogram behind the scenes before data hits the computer vision model.\n",
    "# Still, it is useful to understand what these spectrograms actually look like. Let's \n",
    "# create one ourselves for the same sample clip you have been using.\n",
    "#\n",
    "# 1. Plot a spectrogram of your sample audio file. To do this, \n",
    "#    (i) use librosa.stft (short-time Fourier Transform) to compute the DFT over all short time windows\n",
    "#    (ii) convert amplitude to decibels using librosa.amplitude_to_db (reference the cell above)\n",
    "#         and display it with librosa.specshow.\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26125ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†6b¬†‚Äì¬†Visualizing audio data: Mel spectrograms\n",
    "# ------------------------------------------------------------\n",
    "# A mel spectrogram is a variation of the spectrogram that is commonly used in speech processing and \n",
    "# machine learning tasks. It is similar to a spectrogram in that it shows the frequency content of an \n",
    "# audio signal over time, but on a different frequency axis. In a standard spectrogram, the frequency \n",
    "# axis is linear and is measured in hertz (Hz). However, the human auditory system is more sensitive \n",
    "# to changes in lower frequencies than higher frequencies, and this sensitivity decreases logarithmically \n",
    "# as frequency increases. The mel scale is a perceptual scale that approximates the non-linear frequency \n",
    "# response of the human ear. [Lovely description sourced from HuggingFace audio; I couldn't do any better myself!]\n",
    "#\n",
    "# To create a mel spectrogram, the STFT is used just like before, splitting the audio into short segments \n",
    "# to obtain a sequence of frequency spectra. Additionally, each spectrum is sent through a set of filters, \n",
    "# the so-called mel filterbank, to transform the frequencies to the mel scale. \n",
    "# \n",
    "# 1. Use librosa.feature.melspectrogram to create a mel spectrogram of your sample clip. Again, convert to db\n",
    "#    and display with librosa.display.specshow.\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a9130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†6 - Object-oriented practice\n",
    "# ------------------------------------------------------------\n",
    "# We keep passing around filepaths, audio arrays, dataframe rows, etc. etc.\n",
    "# This is a bit cumbersome. What if we could think of our audio file as a \n",
    "# discrete *object* that we could ask things of (e.g., please show me your spectrogram)?\n",
    "# \n",
    "# This is the idea behind *object-oriented programming* (OOP). We will use OOP extensively\n",
    "# throughout this workshop. As a simple example, let's define an AudioSample *class*\n",
    "# that encapsulates all the functionality we want from an audio file. Then, we can \n",
    "# simply make calls to it later and don't have to duplicate a bunch of code.\n",
    "#\n",
    "# Fill in the class definition below.\n",
    "# ------------------------------------------------------------\n",
    "class AudioSample:\n",
    "    def __init__(self, filepath: Path):\n",
    "        \"\"\"\n",
    "        The __init__ method is called automatically whenever you create an\n",
    "        AudioSample object. We can use this to store *instance variables* \n",
    "        within the object using self.variable_name = variable_value.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "\n",
    "        # We will use the *lazy loading* paradigm: we won't load audio from\n",
    "        # disk until we actually need to use it.\n",
    "        self._audio = None\n",
    "        self._sr = None\n",
    "\n",
    "    def get_audio_and_sr(self):\n",
    "        \"\"\"\n",
    "        Return self._audio and self._sr.\n",
    "        hint: load them (using librosa) if they are not already loaded!\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        return self._audio, self._sr\n",
    "    \n",
    "    def play(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def plot_spectrogram(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "sample_idx = 0\n",
    "sample_file = AudioSample(filepath_from_row(solo_df.iloc[sample_idx]))\n",
    "sample_file.play()\n",
    "sample_file.plot_spectrogram()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoplite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
