{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6172bc",
   "metadata": {},
   "source": [
    "# üê∏¬†AnuraSet¬†Lab‚ÄØ2¬†‚Äî¬†Embeddings & Linear Models \n",
    "\n",
    "\n",
    "At this point you have walked through the steps of the *agile modeling* framework for training machine learning classifiers of audio data. Agile modeling is a specific approach to *(active) transfer learning* -- using an existing machine learning model, which was \"pre-trained\" on some other (possibly unknown) data, to analyze a new set of data.\n",
    "\n",
    "Now, we will depart a bit from agile modeling and implement a transfer learning pipeline ourselves from scratch. To do this, we will continue to utilize a pre-trained model to extract *embeddings* -- low dimensional, semantically meaningful representations -- of new data. In the agile modeling notebooks, we built linear classifiers on top of these embeddings datapoint-by-datapoint, labeling example audio clips one at a time until we obtained a decent classifier. In this notebook, we will instead assume we have already exhaustively labeled a large collection of data. We will try to utilize all these labels, along with the raw audio, to train and test a variety of different classifiers.\n",
    "\n",
    "We will again use *AnuraSet*, a dataset of frog vocalizations, as our test bed. As opposed to the previous notebooks, however, we will *not* be using the 3-second clips and associated metadata. Instead, we will start with raw, 1-minute long audio files, along with species labels of arbitrary duration. We will process these into something that is usable for training. \n",
    "\n",
    "This lab will involve the following:\n",
    "\n",
    "1. **Developing a data loader** that can transform raw audio data into a format that is usable for machine learning: namely, 5-second audio windows with an appropriate sample rate.\n",
    "2. **Extract audio embeddings** from frog recordings using two pretrained networks: YAMNet and Perch.\n",
    "3. Practice **OOP** by wrapping each model behind a common `Embedder` interface.\n",
    "4. **Analyze AnuraSet annotations** to label the species in each 5-second window.\n",
    "5. **Train linear classifiers** (e.g., logistic-regression) with scikit-learn to classify species based on embeddings.\n",
    "\n",
    "Gaps marked **üìù¬†Exercise** should be completed by you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# SETUP\n",
    "# ------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow_hub as hub\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio, display\n",
    "from tqdm.auto import tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8325de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Directory structure\n",
    "# ------------------------------------------------------------\n",
    "DATA_DIR   = Path('/mnt/class_data/anuraset')\n",
    "RAW_DIR    = DATA_DIR / 'raw_data'         # long recordings (.wav)\n",
    "LABEL_DIR  = DATA_DIR / 'strong_labels'    # txt annotation files\n",
    "print('Raw WAV dir  :', RAW_DIR)\n",
    "print('Label txt dir:', LABEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baf6aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†1. Data exploration. \n",
    "# ------------------------------------------------------------\n",
    "# We are working with the raw version of Anuraset now. Let's take a minute to see what we've got.\n",
    "# \n",
    "# 1. Use `os.listdir`, `Path.glob` (https://docs.python.org/3/library/pathlib.html#pathlib.Path.glob), and/or navigate via the command line \n",
    "#       to investigate the contents of RAW_DIR and LABEL_DIR. What do you see?\n",
    "# 2. Load an audio file with librosa.load and display it in this notebook with display(Audio(...)) as in Lab 1.\n",
    "# 3. Load the corresponding label file with pandas and display its head().\n",
    "#       a. Things might look weird. Check out the `sep` keyword of pd.read_csv: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
    "#       b. What do you think the different columns correspond to? Feel free to reference the official documentation of the Anuraset dataset.\n",
    "#       c. Use header=None, and use the `names` parameter to label the columns appropriately.\n",
    "# 4. Find a labeled frog call whose duration is shorter than the total length of the clip.\n",
    "#       a. Extract and display the corresponding sub-clip from the longer clip you loaded in (2)\n",
    "#           Hint: Audio files can be indexed by sample in a similar manner to list indexing in Python. \n",
    "#                 Use the sample rate (from librosa) and the timestamps (in seconds) from the annotation file to get to the correct indices.\n",
    "#       b. Load and display the corresponding sub-clip using librosa.load. \n",
    "#           This is a convenient way to load a short clip without reading the entire WAV file into memory, and may be useful later.\n",
    "#           Hint: Look at the `offset` and `duration` parameters: https://librosa.org/doc/main/generated/librosa.load.html\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7465b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†2. Write a data loader.\n",
    "# ------------------------------------------------------------\n",
    "# Machine learning by definition typically involves reading and processing a lot of data.\n",
    "# Some things must happen before the data is actually given to the model: we need to read\n",
    "# it from disk into memory, (optionally) transfer it to GPU memory, transform it into the\n",
    "# format the model needs (e.g. from raw audio into spectrograms), etc.\n",
    "#  \n",
    "# How to do this efficiently is a challenge. For example, if our dataset contains a lot of large\n",
    "# files, it can take a long time to load them from disk; this causes delays in training\n",
    "# because the model has to wait for all the data to be loaded into memory before it \n",
    "# can take another training step. Well-designed dataloaders minimize this latency.\n",
    "#\n",
    "# We are going to build a dataloader that can efficiently transform the 1-minute-long AnuraSet\n",
    "# audio files into 5-second long audio chunks. We will do this because Perch and YAMNet --\n",
    "# the pre-trained models we will use to extract embeddings -- expect 5-second long audio clips.\n",
    "# This is another thing about data loaders to keep in mind: they are often somewhat model-specific!\n",
    "# If we were instead using a pre-trained audio model that expected spectrograms as input, we \n",
    "# would need to generate those ourselves, taking care to match the exact spectrogram generation\n",
    "# settings that the model was originally trained with. Perch and YAMNet avoid this complication by\n",
    "# accepting raw audio and performing the spectrogram generation internally through what is called an\n",
    "# \"audio frontend\". Still, there are some things we need to match -- namely, audio length and sample rate.\n",
    "#\n",
    "# We will take advantage of Python *generator functions* for this task. A Python generator is like a \n",
    "# lazy function for creating a list: instead of returning all values in the list at once, a generator\n",
    "# \"yield\"s one item at a time and then stops, keeping track of its state, and does no further\n",
    "# computation until it is asked for the next item. We can use this as follows: for each one-minute audio\n",
    "# file, instead of processing the whole thing at once and generating 12 5-second audio clips,\n",
    "# we can quickly \"yield\" the first clip and wait until the second one is requested, then \"yield\" that\n",
    "# one and wait, and so on.\n",
    "#\n",
    "# Like most things, this is easiest to understand by actually implenting it. Complete the audio_chunk_dataloader\n",
    "# (a generator function) below as follows:\n",
    "#\n",
    "# 1. Use glob to get a list of all audio files that match the `file_glob``. This glob will look something like\n",
    "#    path/to/audio/*.wav. If `shuffle`` is true, shuffle the order of this list.\n",
    "#\n",
    "# 2. For each file, iterate in `window_size_s`-second chunks and `yield` a tuple containing:\n",
    "#    (audio, filepath, offset), where `audio` is a `window_size_s`-second audio file at `target_sample_rate`,\n",
    "#    `filepath` is the path to the full audio file, and `offset` is the start time in seconds of the chunk.\n",
    "#     Hint: rather than loading the big audio files from disk, use librosa.get_duration with the `path` argument \n",
    "#     to figure out the number of chunks in a each audio file, then load each chunk separately (as above) with librosa.load.\n",
    "#     This will make it easier to parallelize things later, since we won't have to wait for a big audio file\n",
    "#     to load every few data batches.\n",
    "#     Hint #2: Some files are not exactly 60s long, they may be something like 59.6s long. Make sure you \n",
    "#     handle this when calling librosa.load on the last chunk of the file.\n",
    "# ------------------------------------------------------------\n",
    "def audio_chunk_dataloader(file_glob, target_sample_rate, window_size_s, shuffle=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file_glob: str, glob pattern for the audio files\n",
    "        target_sample_rate: int, sample rate to load the audio files\n",
    "        window_size_s: float, size of the audio chunks in seconds\n",
    "        shuffle: bool, whether to shuffle the order of the audio files in the glob; use a fixed seed for reproducibility\n",
    "    Yields:\n",
    "        audio_window: np.ndarray, audio chunk\n",
    "        file: str, path to the audio file\n",
    "        offset: float, start time of the audio chunk in seconds\n",
    "\n",
    "    Hint:\n",
    "    for file in glob:\n",
    "      for audio_window in file:\n",
    "        yield audio_window, file, offset\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "## Test:\n",
    "file_glob = str(RAW_DIR / '**/*.wav')\n",
    "target_sample_rate = 16000\n",
    "window_size_s = 5.0\n",
    "dataloader = audio_chunk_dataloader(file_glob, target_sample_rate, window_size_s, shuffle=True)\n",
    "num_to_test = 15\n",
    "for i, (audio_chunk, file, offset) in tqdm(enumerate(dataloader)):\n",
    "    print(f\"Loaded {file} at {offset:.2f}s\")\n",
    "    display(Audio(audio_chunk, rate=target_sample_rate))\n",
    "    if i > num_to_test: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ea1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†3.1 Chunked inference on a single file with YAMNet.\n",
    "# ------------------------------------------------------------\n",
    "# Let's get some embeddings! We will do this for a single file first.\n",
    "# We will iterate over all chunks in this file using our new dataloader, \n",
    "# and ask our pre-trained model to perform inference on each audio chunk.\n",
    "#\n",
    "# Let's start with the YAMNet model. Google has provided instructions for\n",
    "# how to run inference here: https://www.tensorflow.org/hub/tutorials/yamnet#executing_the_model\n",
    "# These will need to be modified slightly to fit into our approach; \n",
    "# in particular you can ignore most of their data setup and focus on the \n",
    "# \"Executing the Model\" section. \n",
    "#\n",
    "# 1. Fill in the method below to extract an embedding from each chunk, and \n",
    "#    append this embedding to the embeddings list.\n",
    "#\n",
    "# Note that YAMNet internally splits each chunk into 0.48s 'frames' and returns one embedding for each.\n",
    "# We will follow the following approach:\n",
    "# \"When a model‚Äôs window size is shorter than a target example, we frame the audio according to the model‚Äôs window size, \n",
    "# create an embedding for each frame, and then average the results\" https://www.nature.com/articles/s41598-023-49989-z\n",
    "#\n",
    "# Hint: this involves taking a mean over all the embeddings of all the frames for each chunk. Make sure\n",
    "# your tensor shape looks correct after performing this mean: each embedding should have shape (1024,)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def inference_yamnet():\n",
    "    CHUNK_LEN_S = 5     # use 5s chunks\n",
    "    TARGET_SR = 16_000  # YAMNet expects a 16khz sample rate\n",
    "\n",
    "    # load the model\n",
    "    yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "\n",
    "    # for testing, lets just look at one file\n",
    "    # note this is a valid file_glob for creating our dataloader!\n",
    "    sample_file = '/mnt/class_data/anuraset/raw_data/INCT20955/INCT20955_20191031_030000.wav'\n",
    "\n",
    "    # iterate through the file in CHUNK_LEN_S chunks and append each embeddding to this list\n",
    "    embeddings = []\n",
    "    dataloader = audio_chunk_dataloader(sample_file, TARGET_SR, CHUNK_LEN_S)    # note a single filepath is also a valid glob pattern\n",
    "    for chunk, _, _ in dataloader:                                              # we don't need the filepath and offset now, so use dummy variable _ to ignore them\n",
    "        # Fill this in\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # stack the embeddings into one big array\n",
    "    embeddings = np.stack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Test:\n",
    "assert inference_yamnet().shape == (12, 1024), \"YAMNet embeddings should be of shape (12, 1024) for a 1 minute clip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†3.2 Chunked inference on a large file with Perch.\n",
    "# ------------------------------------------------------------\n",
    "# Now let's do the same thing for the Perch model. You can reference some example code here:\n",
    "# https://www.kaggle.com/models/google/bird-vocalization-classifier/tensorFlow2/bird-vocalization-classifier/4?tfhub-redirect=true\n",
    "# to see how inference with Perch differs from inference with YAMNet.\n",
    "#\n",
    "# Notably, Perch expects chunks that are *exactly* 5 seconds long. Some of the files in our\n",
    "# dataset are annoyingly *not* exactly 60 seconds long; they are something like 59.6 seconds long.\n",
    "# We provide a convenience method below to pad any numpy array to a given size, which you can\n",
    "# use to tackle this.\n",
    "#\n",
    "# 1. Fill in the inference_perch() method below. The return type should be the same as inference_yamnet(),\n",
    "# except the embedding size is different; each embedding should be shape (1280,)\n",
    "\n",
    "def zeropad(A, size):\n",
    "    \"\"\"Convenience function to pad a numpy array to a given size. Useful for Perch, which requires 5s chunks.\"\"\"\n",
    "    t = size - len(A)\n",
    "    return np.pad(A, pad_width=(0, t), mode='constant')\n",
    "\n",
    "def inference_perch():\n",
    "    CHUNK_LEN_S = 5\n",
    "    TARGET_SR = 32_000  # Perch is 32khz\n",
    "\n",
    "    perch_model = hub.load('https://www.kaggle.com/models/google/bird-vocalization-classifier/TensorFlow2/bird-vocalization-classifier/4')\n",
    "    sample_file = '/mnt/class_data/anuraset/raw_data/INCT20955/INCT20955_20191031_030000.wav'\n",
    "\n",
    "    embeddings = []\n",
    "    dataloader = audio_chunk_dataloader(sample_file, TARGET_SR, CHUNK_LEN_S)\n",
    "    for chunk, _, _ in dataloader:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    embeddings = np.stack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# Test:\n",
    "assert inference_perch().shape == (12, 1280), \"Perch embeddings should be of shape (12, 1280) for a 1 minute clip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†4. OOP practice\n",
    "# ------------------------------------------------------------\n",
    "# We duplicated a lot of code in the above two methods. Most of the logic for\n",
    "# running inference is exactly the same regardless of which model you are\n",
    "# using to extract embeddings. The only difference is the model-specific logic\n",
    "# for running inference. This sounds like a great fit for an object-oriented design!\n",
    "#\n",
    "# We will define an Embedder class to encapsulate this model-specific logic.\n",
    "# First, we will define a base class (AKA \"parent class\") called `Embedder`; \n",
    "# then we can create \"child classes\" for both YAMNet and Perch that extend from Embedder.\n",
    "# Later code can treat them exactly the same.\n",
    "#\n",
    "# 1. Fill in the __init__ and __call__ methods of YAMNetEmbedder and PerchEmbedder.\n",
    "# This involves setting the instance variables self.target_sr and self.model to \n",
    "# appropriate values for each; then filling in inference code. See test_perch_embedder()\n",
    "# below for an example of how these methods will be called.\n",
    "# Hint: you shouldn't really need to write any new code if you implemented the above\n",
    "# methods correctly...\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Don't modify this class! It is purposely left NotImplemented to indicate to subclasses\n",
    "# what the expected interface should be.\n",
    "class Embedder:\n",
    "    def __init__(self):\n",
    "        # You don't need to modify this!\n",
    "        # Override these in subclasses:\n",
    "        self.target_sr = -1\n",
    "        self.model = None\n",
    "\n",
    "    def __call__(self, audio_chunk):\n",
    "        \"\"\"\n",
    "        You don't need to modify this! \n",
    "        Subclasses are required to override this logic.\n",
    "\n",
    "        Args:\n",
    "            audio_chunk: np.ndarray, shape (chunk_size_in_seconds*sample_rate,), the audio chunk to embed\n",
    "        Returns:\n",
    "            embedding: np.ndarray, shape (embedding_dim,), the embedding of the audio chunk\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class YAMNetEmbedder(Embedder):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, audio_chunk):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class PerchEmbedder(Embedder):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, chunk):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Test\n",
    "def test_perch_embedder():\n",
    "    embedder = PerchEmbedder()\n",
    "    sample_file = '/mnt/class_data/anuraset/raw_data/INCT20955/INCT20955_20191031_030000.wav' # example\n",
    "    dataloader = audio_chunk_dataloader(sample_file, embedder.target_sr, embedder.chunk_size_s)\n",
    "    \n",
    "    for chunk, _, _ in dataloader:\n",
    "        embedding = embedder(chunk)\n",
    "        print(embedding.shape)\n",
    "        assert embedding.shape == (1280,)\n",
    "        break  # Only test the first chunk\n",
    "    print(\"PerchEmbedder test passed!\")\n",
    "\n",
    "test_perch_embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b60c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†5. Extract embeddings and file information\n",
    "# ------------------------------------------------------------\n",
    "# Let's start generating a dataset of embeddings that we can use to train a species classifier.\n",
    "# We will start small, and generate embeddings for 100 audio chunks. Then, (in the next exercise)\n",
    "# we will extract labels for these files; finally (in a couple more exercises) we will use\n",
    "# these embeddings+labels to train a classifier.\n",
    "#\n",
    "# 1. Using your dataloader and Embedder classes, generate Perch embeddings of all audio chunks at SITE.\n",
    "#    Use shuffle=True in your dataloader to get a diverse set at the front of the list.\n",
    "#    Put results into two lists:\n",
    "#    embeddings -> list of np.ndarray, each of shape (embedding_dim,)\n",
    "#    file_info -> list of tuples, each is (filepath, offset_s)\n",
    "#        this will allow us to remember where each embedding came from so that we can associate each \n",
    "#        embedding with a species label later\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# for now, just embed a subset (of MAX_FILES) at a sample site for proof of concept (to reduce wait time)\n",
    "SITE = 'INCT17'\n",
    "MAX_CHUNKS = 100\n",
    "CHUNK_LEN_S = 5\n",
    "print(\"Embedding\", MAX_CHUNKS, \"audio snippets of length\", CHUNK_LEN_S, \"seconds at site\", SITE)\n",
    "\n",
    "embeddings = []\n",
    "file_info = []\n",
    "embedder = ...\n",
    "dataloader = ...\n",
    "# then: for ... in dataloader: ...\n",
    "\n",
    "# Test\n",
    "embeddings = np.stack(embeddings)\n",
    "assert embeddings.shape == (100,1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†6. Extract labels for binary classification \n",
    "# ------------------------------------------------------------\n",
    "# In order to train a model to classify the species in each audio chunk,\n",
    "# we need species labels for each chunk. For now, we will pick a single\n",
    "# target species, and train a \"binary classifier\", i.e., a classifier\n",
    "# with only two classes. In our case, the classes are \"our target species is\n",
    "# not present\" vs. \"our target species is present\". We will give the former\n",
    "# the class label of 0, and the latter the class label of 1. (i.e, a 1 means\n",
    "# our species of interest is present in the clip.)\n",
    "# \n",
    "# Above, you explored what the label files look like: species labels with time\n",
    "# ranges of when they are present. Now we will process these to determine, for\n",
    "# each audio chunk returned by our data loader, if the label should be a 0 or 1\n",
    "# for our target species.\n",
    "#\n",
    "# For each audio chunk we have embedded,\n",
    "# 1. Using the file information, find and load the associated label file using pandas.\n",
    "#    Hint: you did this at the beginning of this notebook.\n",
    "# 2. Each label file represents an entire one-minute audio file. Filter the pandas\n",
    "#    dataframe down to labels that apply to the current 5-second chunk.\n",
    "#    Hint: remember that the dataloader returns an `offset` that tells you where the\n",
    "#    current chunk is located in the longer audio file.\n",
    "# 3. Append either a 0 or 1 to the `ys` list depending on whether your target_species\n",
    "#    is in this chunk.\n",
    "# ------------------------------------------------------------\n",
    "target_species = 'DENMIN_M'\n",
    "\n",
    "ys = []\n",
    "for i, (file, offset) in enumerate(file_info):\n",
    "    # Fill this in\n",
    "    pass\n",
    "\n",
    "# Test: make sure we have one label per embedding\n",
    "assert len(ys) == embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2eacb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†6.1 Verify you don't have any bugs!\n",
    "# ------------------------------------------------------------\n",
    "# Use the audio visualization code from the first notebook to check a few positive \n",
    "# and negative examples of your target species. Does everything look (/sound)\n",
    "# as expected?\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e30bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†7. Train a simple logistic regression model \n",
    "# ------------------------------------------------------------\n",
    "# At long last, we are ready to train a model to classify embeddings\n",
    "# as containing our target_species or not.\n",
    "#\n",
    "# We will use scikit-learn to do so: https://scikit-learn.org/stable/\n",
    "# \n",
    "# 1. First, we need to decide which data we will use to \"train\" the model,\n",
    "#    and which data we will use to \"test\" the model, i.e. determine whether\n",
    "#    the training process is going well. Create a simple random train/test \n",
    "#    split using sklearn's train_test_split function, Stratify by class labels \n",
    "#    (ys) to ensure that the training and test sets have the same proportion of \n",
    "#    positive and negative examples.\n",
    "#    Food for thought: This method randomly splits our data into train and test. \n",
    "#       When is this appropriate or inappropriate to do? Does this give us an \n",
    "#       accurate picture of how the model will perform on future data? Why or why not?\n",
    "#\n",
    "# 2. Train a logistic regression model. There are lots of tutorials out there for \n",
    "#    how to do this with scikit learn; the imports in this cell also give you some hints.\n",
    "#    The X's will be our embeddings; the Y's will be our labels.\n",
    "#\n",
    "# 3. Evaluate your trained model on the test set. Print the ROC-AUC score on the test set.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# there are hints hidden in these imports...\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†8. Train KNN\n",
    "# ------------------------------------------------------------\n",
    "# In logistic regression, \"training\" inolves drawing a decision boundary\n",
    "# between the 0 class and the 1 class in the embedding space. \n",
    "# \"Inference\" then involves just determining which side of this boundary\n",
    "# a new test point lies on. This is a \"parametric\" model, since the decision\n",
    "# boundary is defined by slope and intercept parameters.\n",
    "#\n",
    "# An alternate, non-parametric approach is a \"nearest neighbors\" classifier.\n",
    "# This is basically what it sounds like: when performing inference, look at our\n",
    "# training set (the points we have labels for). Find the training examples\n",
    "# that are closest to the new test point in the embedding space, and just go ahead\n",
    "# and say the new test point is probably the same class as those! This doesn't\n",
    "# even really require \"training\", as no parameters are fit. (The number of\n",
    "# neighbors to consider is a \"hyperparameter\" that is set by the user rather\n",
    "# than learned from data).\n",
    "#\n",
    "# 1. Create, fit, and evaluate a KNeighborsClassifier on the same X's and Y's as above.\n",
    "#    Print the ROC-AUC score. \n",
    "#    Hint: There are two ways you can predict in scikit-learn: predict() and predict_proba(). \n",
    "#    How does the choice of prediction affect your ROC-AUC score? Why? Which one is more\n",
    "#    appropriate to use?\n",
    "#\n",
    "# ------------------------------------------------------------\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù¬†Exercise¬†8. Train a model on all the data!\n",
    "# ------------------------------------------------------------\n",
    "# TODO:\n",
    "# Point to embeddings we've extracted for everywhere\n",
    "# Train/test splits: random vs. sites\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional extensions\n",
    "# ------------------------------------------------------------\n",
    "# 1. Try out different classifiers (e.g. SVM, Random Forest, NN, etc.) and see how they perform.\n",
    "# 2. Try out different embedding models (e.g. YAMNet, Perch, etc.) and see how they perform.\n",
    "# 4. Try out different data augmentation techniques (e.g. pitch shifting, time stretching, etc.) and see how they affect the performance of your classifier.\n",
    "# 5. Try out different hyperparameters (e.g. learning rate, batch size, etc.) and see how they affect the performance of your classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hoplite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
