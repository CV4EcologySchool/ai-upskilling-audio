{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100aec12",
   "metadata": {},
   "source": [
    "# Introduction to Vector Search\n",
    "\n",
    "Vector Search might sound scary and complicated, but it's actually really easy! And then really hard and complicated!\n",
    "\n",
    "We computed *embeddings* of audio snippets. Now, given some example, we want to find similar audio snippets in our dataset.  To do this, we'll use vector search over the embeddings.\n",
    "\n",
    "We'll start by doing *brute force nearest-neighbor search*. This is conceptually very easy: Just compare the query embedding to all of the target embeddings, and return the closest one(s).\n",
    "\n",
    "We'll also find that we need to have some good data mangement along the way - it's not enough to get the best embedding, we also need to know what audio it is associated with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d08058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù Exercise 0 ‚Äì Create a query.\n",
    "# ------------------------------------------------------------\n",
    "# This should mostly use code you've already written for\n",
    "# previous exercises.\n",
    "# 1) Create a variable for a file-path to some audio.\n",
    "# 2) Load the audio.\n",
    "# 3) Run the audio through an embedding model (Perch?) and get\n",
    "#    the audio embedding. This is your query embedding.\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a2f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù Exercise 1 ‚Äì Create an embeddings dataset.\n",
    "# ------------------------------------------------------------\n",
    "# This also should rely heavily on code you've already \n",
    "# written for previous exercises... but will go a bit further.\n",
    "# 1) Get all the embeddings for your dataset (eg, your \n",
    "#    favorite anuraset site, or all of the anuraset data).\n",
    "# 2) Write the embeddings to disk, somehow. Check that you\n",
    "#    can reload the embeddings from disk, and that the \n",
    "#    reloaded embeddings match the originals.\n",
    "# 3) Given some choice of embedding from your dataset,\n",
    "#    write a function which gets the audio that the embedding\n",
    "#    came from.\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a41d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù Exercise 2 ‚Äì Nearest-neighbor search.\n",
    "# ------------------------------------------------------------\n",
    "# Now we should have a *query embedding* and a numpy array \n",
    "# of *target embeddings.*\n",
    "# 1) Write a function which compares the query embedding to\n",
    "#    each of the target embeddings, and finds the one that is\n",
    "#    closest in Euclidean distance. Return the embedding and\n",
    "#    the associated audio.\n",
    "# 2) Update your function to take a `top_k` parameter. Then\n",
    "#    it should return a numpy array of embeddings (with shape\n",
    "#    [top_k, embedding_dim]) and an array of audio (with shape\n",
    "#    [top_k, 5*sample_rate]).\n",
    "# 3) Update your function to have some options for how to \n",
    "#    the nearest neighbor - try out cosine similarity and\n",
    "#    maximum inner product.\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fda6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù Exercise 3 ‚Äì Displaying results.\n",
    "# ------------------------------------------------------------\n",
    "# Now that you have some top_k results, display them!\n",
    "# Draw a spectrogram and audio player for each result.\n",
    "# It's also helpful to write the filename and offset within\n",
    "# the file for each result, to make it possible to go\n",
    "# back and see the result in context.\n",
    "#\n",
    "# Extensions:\n",
    "# E1) Provide a button for 'relevant' / 'irrelevant'.\n",
    "# E2) Skip results which have already been marked.\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b24847",
   "metadata": {},
   "source": [
    "## Vector Databases in a Nutshell\n",
    "\n",
    "Brute force search is fantastic (and fast!) until you get to millions \n",
    "or billions of embeddings.\n",
    "\n",
    "(Micro-exercise: A 32-bit floating point number takes 4 bytes. \n",
    "How many embeddings can fit in RAM in the machine you're working on?\n",
    "How many hours of audio does that number of embeddings correspond to?\n",
    "How long would it take to run a brute-force search on that many embeddings?)\n",
    "\n",
    "When the number of embeddings becomes enormous, vector databases become\n",
    "sort of helpful!\n",
    "\n",
    "The idea of a vector database is to find *approximate* nearest neighbors\n",
    "quickly. This is done by *indexing* the data. There is an enormous literature\n",
    "on ways to do this well, but a good cartoon-version of what works well is \n",
    "*hierarchical k-means*. You cluster the data into k clusters, then cluster\n",
    "all the data assigned to each cluster centroid, and so on. Then to find \n",
    "nearest neighbors, you find the nearest top-centroid, then the nearest centroid\n",
    "at the second level, and so on.\n",
    "\n",
    "(More info than you need: this hierarchical k-means procedure has trouble when\n",
    "searching for something near the cluster boundary. Various tricks can be introduced\n",
    "to deal with this. There is also a family of *graph-based* indices, which\n",
    "are really cool mathematically and work pretty great, but deep in the weeds.)\n",
    "\n",
    "In my opinion, the best vector database in 2025 is called **usearch**. It's great because:\n",
    "\n",
    "* It is self-contained, with almost no dependencies: It has one job, and it does it well.\n",
    "* It can use an *on-disk* index! Most of the popular vector databases only work with embeddings in RAM, in order to give results as fast as possible. Using an on-disk index lets you scale to a much larger number of vectors for far less money.\n",
    "\n",
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a294c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 9s, sys: 427 ms, total: 2min 9s\n",
      "Wall time: 17.9 s\n",
      "100000\n",
      "\n",
      "time to run exact search: \n",
      "CPU times: user 18.1 ms, sys: 1 Œºs, total: 18.1 ms\n",
      "Wall time: 18 ms\n",
      "\n",
      "time to run approximate search: \n",
      "CPU times: user 1.02 ms, sys: 0 ns, total: 1.02 ms\n",
      "Wall time: 1.02 ms\n",
      "\n",
      "time to run approximate search from disk: \n",
      "CPU times: user 3.46 ms, sys: 3.99 ms, total: 7.44 ms\n",
      "Wall time: 7.44 ms\n"
     ]
    }
   ],
   "source": [
    "from usearch import index as uindex\n",
    "import numpy as np\n",
    "\n",
    "# For \"free\", here's an example of how to use usearch to index \n",
    "# and search some vectors.\n",
    "\n",
    "# Create the index, specifying the size of the vectors, \n",
    "# data type (dtype), and the metric to use.\n",
    "ui = uindex.Index(ndim=512, metric=\"L2sq\", dtype='f16')\n",
    "\n",
    "# Make some random data.\n",
    "n = 100_000\n",
    "keys = np.arange(n)\n",
    "vectors = np.random.rand(n, 512).astype(np.float32)\n",
    "print('\\ntime to create the index: ')\n",
    "%time ui.add(keys, vectors)\n",
    "\n",
    "print(len(ui.keys))\n",
    "\n",
    "# Make a random query.\n",
    "query = np.random.rand(512).astype(np.float32)\n",
    "\n",
    "print('\\ntime to run exact search: ')\n",
    "%time exact_top_k = ui.search(query, count=5, exact=True)\n",
    "\n",
    "print('\\ntime to run approximate search: ')\n",
    "%time approx_top_k = ui.search(query, count=5, exact=False)\n",
    "\n",
    "# TODO: Write some code measuring the proportion of exact_top_k\n",
    "# that are in approx_top_k. This is a *recall* metric.\n",
    "\n",
    "# Save the index to disk.\n",
    "ui.save(\"/tmp/index.bin\")\n",
    "\n",
    "# Use the on-disk index.\n",
    "ui2 = uindex.Index()\n",
    "ui2.view(\"/tmp/index.bin\")\n",
    "# Check that the loaded index is the same as the original.\n",
    "assert np.all(np.array(ui2.keys) == np.array(ui.keys))\n",
    "\n",
    "print('\\ntime to run approximate search from disk: ')\n",
    "%time approx_disk_top_k = ui2.search(query, count=5, exact=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c54aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù Exercise 4 ‚Äì Embeddings usearch.\n",
    "# ------------------------------------------------------------\n",
    "# Do all the same stuff again, but this time with embeddings\n",
    "# from the Perch model.\n",
    "#\n",
    "# 1) Create a usearch index for the embeddings, and insert\n",
    "#    the embeddings into it.\n",
    "# 2) Save the index to disk, and load it again.\n",
    "# 3) Create a query embedding, and search the index for the\n",
    "#    nearest neighbors.\n",
    "# 4) Display the results, with a spectrogram of the query\n",
    "#    and the nearest neighbors.\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b80d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù Exercise 5 ‚Äì Measuring quality.\n",
    "# ------------------------------------------------------------\n",
    "# Those who develop search are doomed to try to measure search\n",
    "# quality. There's a lot of depth in this question, but there\n",
    "# is at least a simple mechanical measure of recall which is\n",
    "# useful for comparing approximate nearest neighbor search to\n",
    "# exact nearest neighbor search.\n",
    "#\n",
    "# Search quality is typically measured by recall methods. One\n",
    "# common metric is \"recall@k\", which is the proportion of the\n",
    "# top k exact-search results that are in the top k results of \n",
    "# the approximate nearest neighbor search.\n",
    "#\n",
    "# 1) Write a function which takes a query embedding and a\n",
    "#    usearch index, and returns the recall@k for that query.\n",
    "# 2) Write a function which takes a usearch index, selects a \n",
    "#    set of N random queries, and returns the mean recall@k for \n",
    "#    those queries.\n",
    "# ------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a592015",
   "metadata": {},
   "source": [
    "## Limitations of ANNS\n",
    "\n",
    "Approximate Nearest Neighbor Search does exactly one thing really well: Find the most similar stuff in a big corpus of stuff. But there are a couple key limitations to be aware of.\n",
    "\n",
    "There are two big limitations:\n",
    "\n",
    "1) Filtering by covariates. We often really want to ask \"What's the most similar audio from this specific site / time of day / etc?\"\n",
    "2) Inflexibility. The ANNS search index is built against one specific metric, to get the largest results. For large datasets, the index can take a long time to create. And we can't look for 'things with a score near 0' easily.\n",
    "\n",
    "\n",
    "### Filtering by Covariates\n",
    "\n",
    "There are a few algorithms (eg, DiskANN) which have some ability to build some particular covariates into the search index, allowing some filtering for specific kinds of metadata that you identify in advance of building the index. But if you want to add more kinds of filtering later, you need to rebuild the index. And this is all bleeding-edge research anyway.\n",
    "\n",
    "There are two somewhat reasonable workarounds:\n",
    "\n",
    "* Do a giant ANNS search and then filter the results according to the metadata. For example, search for the top-1000, then filter/discard everything that doesn't match the parameters I care about, and return the top-5 of whatever's left. (The disadvantage is that you may get zero results back, if there's nothing in the top-1000 results matching the metadata you want.)\n",
    "\n",
    "* Get all of the embeddings corresponding to the metadata, and then use brute-force search on the results. (The downside is that this might be slow if there's a whole lot of data matching the metadata.)\n",
    "\n",
    "### Sampled Brute Force Search\n",
    "\n",
    "Depending on the application, sometime sampled brute-force search is still a good idea. For margin sampling (ie, looking for classifier results with logit near 0), ANNS can't really help (you're not looking for the top-value, and you don't want to re-index to find things that are ambiguous).\n",
    "\n",
    "Instead, you can take a random subset of embeddings and use brute-force search over that subset. Note that the recall of sampled random search is going to be approximately equal to the sample size. eg, if I take a 50% slice of the data and apply brute force search, I will on average find about 50% of the top-k results. So this can be helpful when you dont' need high recall for your ultimate goal, but instead just want something to work with quickly. Evaluating negatives or ambiguous examples is thus a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7af089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# üìù Exercise 4 ‚Äì Metadata Filtering\n",
    "# ------------------------------------------------------------\n",
    "# First, try some post-filtering. Write a function which\n",
    "# takes the site-name (eg, 'INCT17') and a query embedding,\n",
    "# runs a usearch search, and then filters the results to\n",
    "# only include results from the same site.\n",
    "#\n",
    "# Then, try some pre-filtering. Write a function which\n",
    "# takes a site-name and a query embedding. Get all of the\n",
    "# embeddings from that site, and use brute-force search\n",
    "# to find the nearest neighbors. \n",
    "# \n",
    "# Compare the results of the two approaches.\n",
    "# ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247e24d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
