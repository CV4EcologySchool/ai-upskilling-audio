{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fasterer Python\n",
        "\n",
        "Pure Python is slow, for two reasons. First, it is an **interpreted language**, which is always less efficient than a compiled language. Second, Python was originally conceived to work on a single thread, to make programming easier. It has a **Global Interpreter Lock (GIL)**, which means that only one python instruction is carried out at a time.\n",
        "\n",
        "However, there are easy ways to make Python code run **very fast**, circumventing these problems.\n",
        "\n",
        "First, Python is very good at interfacing with non-Python libraries. This means you can write fast, compiled code in Rust or C++, and then call it from Python. The compiled libraries are exempt from the GIL, meaning that they can do their own threading, as well.\n",
        "\n",
        "Second, there are a couple forms of parallelism within Python which circumvent the GIL.\n",
        "\n",
        "## Numpy\n",
        "\n",
        "Numpy (\"Numerical Python\") is a fast library for doing all kinds of numerical array operations in Python. It's written in an unholy combination of C++ and Fortran, but as a user, you never have to worry about that!\n",
        "\n",
        "The golden rule of Numpy is that using numpy operations on numpy arrays is going to be fast, but using Python operations will be slow. For example, iterating over a matrix with for loops is slow, but operating on a matrix with numpy operations is fast.\n",
        "\n",
        "Let's see an example..."
      ],
      "metadata": {
        "id": "fhiAY5NC8cCC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMzspJll8Zgv"
      },
      "outputs": [],
      "source": [
        "# Everyone calls numpy np.\n",
        "import numpy as np\n",
        "\n",
        "# Make a matrix of random numbers.\n",
        "mat = np.random.default_rng().random((1024 * 1024, 2), dtype=np.float32)\n",
        "\n",
        "# Use a loop to multiply it by 2.\n",
        "def times_two(mat):\n",
        "    for i in range(mat.shape[0]):\n",
        "        for j in range(mat.shape[1]):\n",
        "            mat[i, j] *= 2\n",
        "    return mat\n",
        "\n",
        "# Use numpy to multiply it by 2.\n",
        "def times_two_numpy(mat):\n",
        "    return mat * 2\n",
        "\n",
        "# On tmd@'s computer, the times_two_numpy is ~500x faster.\n",
        "%time mat = times_two(mat)\n",
        "%time mat = times_two_numpy(mat)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numpy Basics\n",
        "\n",
        "* Most of the time, numpy works with *arrays*.\n",
        "* Arrays are \"rectangular\", with any number of *axes*.\n",
        "* You can get the size of an array with the `.shape` attribute.\n",
        "* Often you can perfrom an operation on a particular axis: eg, `np.mean(X, axis=3)`.\n",
        "* When combining two arrays, you can often *broadcast* an operation if the shapes match except some axes with size 1.\n",
        "\n",
        "### Extremely useful operations\n",
        "\n",
        "* Construct arrays\n",
        "  * `np.ones`, `np.zeros` - make constant one/zero arrays\n",
        "  * `np.eye` - make an identity matrix.\n",
        "* Common mathy operations\n",
        "  * `np.mean`, `np.std`, `np.maximum`, `np.minimum`, etc...\n",
        "  * `np.linalg.norm` - L2 / Euclidean norm.\n",
        "  * `np.sort`, `np.argsort`\n",
        "  * `np.dot` (dot product), `np.matmul`\n",
        "* Manipulate arrays\n",
        "  * `np.concat` - Concatenate on some axis. (eg, combine arrays with shapes [2, 3], [3, 3] to a single array with shape [5, 3] by concat'ing along the 0th axis.)\n",
        "  * `np.stack` - Stack arrays with matching shape along a new axis. (eg,  combine arrays with shapes [4, 3], [4, 3] to a new array with shape [2, 4, 3].)\n",
        "  * `np.reshape` - Re-organize the shape of an array, eg [10, 8] -> [2, 5, 8].\n",
        "  * `np.transpose` - Swap axes.\n",
        "* Slices\n",
        "  * Slices notation can select a sub-array, eg `X[0:4]` selects the first three rows of `X`. You can slice along multiple axes (`X[0:4,\n"
      ],
      "metadata": {
        "id": "ZwZ8v7ge464A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.round(np.random.uniform(size=(3, 3)), 2)\n",
        "print(X)\n",
        "\n",
        "print('\\nsorted on axis 0 (cols): ')\n",
        "print(np.sort(X, axis=0))\n",
        "\n",
        "print('\\nsorted on axis 1 (rows): ')\n",
        "print(np.sort(X, axis=1))\n",
        "\n",
        "print('\\nindices of sorted cols: ')\n",
        "print(np.argsort(X, axis=0))\n",
        "\n",
        "np.dot()"
      ],
      "metadata": {
        "id": "7FUwdvnR45lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Tokenizer\n",
        "\n",
        "Let's get the hangs of working with common Numpy operations by writing a small **tokenizer**.\n",
        "\n",
        "A tokenizer converts vectors to discrete objects... It's a fancy way of saying that we'll do some clustering, and number a vector according to which cluster it wound up in.\n",
        "\n",
        "There are approximately a thousand ways to do clustering, but it turns out that k-means is still pretty damned useful, and more-or-less what people do in practice. So, let's do some k-means clustering."
      ],
      "metadata": {
        "id": "XHFvQ9BJDcMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_initial_centroids(data: np.ndarray, k: int) -> np.ndarray:\n",
        "  \"\"\"Generate initial centroids for k-means clustering.\n",
        "\n",
        "  Args:\n",
        "    data: The data to cluster.\n",
        "    k: The number of clusters to generate.\n",
        "\n",
        "  Returns:\n",
        "    The initial centroids.\n",
        "  \"\"\"\n",
        "  return data[np.random.choice(data.shape[0], k, replace=False)]\n",
        "\n",
        "\n",
        "def assign_to_clusters(data: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"Assign data points to clusters.\n",
        "\n",
        "  Args:\n",
        "    data: The data to cluster.\n",
        "    centroids: The centroids to use for clustering.\n",
        "\n",
        "  Returns:\n",
        "    Array with numerically assigned clusters.\n",
        "  \"\"\"\n",
        "  distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n",
        "  return np.argmin(distances, axis=1)\n",
        "\n",
        "\n",
        "def update_centroids(data: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"Update the centroids to be the mean of the data points in each cluster.\n",
        "\n",
        "  Args:\n",
        "    data: The data to cluster.\n",
        "    centroids: The centroids to use for clustering.\n",
        "\n",
        "  Returns:\n",
        "    The updated centroids.\n",
        "  \"\"\"\n",
        "  assignments = assign_to_clusters(data, centroids)\n",
        "  for i in range(centroids.shape[0]):\n",
        "    centroids[i] = np.mean(data[assignments == i], axis=0)\n",
        "  return centroids\n",
        "\n",
        "def reconstruction_loss(data: np.ndarray, centroids: np.ndarray) -> float:\n",
        "  \"\"\"Compute the mean distance to the closest centroid.\n",
        "\n",
        "  Args:\n",
        "    data: The data to cluster.\n",
        "    centroids: The centroids to use for clustering.\n",
        "\n",
        "  Returns:\n",
        "    The mean distance to the closest centroid.\n",
        "  \"\"\"\n",
        "  distances = np.linalg.norm(data[:, np.newaxis] - centroids, axis=2)\n",
        "  return np.mean(np.min(distances, axis=1))"
      ],
      "metadata": {
        "id": "kNQ4u7YuC1jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 64\n",
        "n = 1024 * 8\n",
        "\n",
        "mat = np.random.default_rng().random((n, 2), dtype=np.float32)\n",
        "%time centroids = generate_initial_centroids(mat, k)\n",
        "print(centroids.shape)\n",
        "\n",
        "import tqdm\n",
        "pbar = tqdm.tqdm(range(100))\n",
        "for i in pbar:\n",
        "  assign_to_clusters(mat, centroids)\n",
        "  update_centroids(mat, centroids)\n",
        "  loss = reconstruction_loss(mat, centroids)\n",
        "  pbar.set_description(f'loss: {loss:8.3f}')"
      ],
      "metadata": {
        "id": "SjFx0nh5equN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "colors = assign_to_clusters(mat, centroids)\n",
        "\n",
        "plt.scatter(mat[:, 0], mat[:, 1], c=colors, alpha=0.1)\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='r', marker='*', s=100.0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q7soot1geycc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra challenges...\n",
        "\n",
        "* Usually, you can't fit all the data in memory. In that case, you want to do *partial updates* of the cluster centroids. Change the `update_centroids` to take a step-size parameter. Replace each centroid with a weighted average of the current centroid and the mean of the points assigned to the centroid.\n",
        "\n",
        "* A problem for k-means clustering on real data is 'dead' centroids. Come up with a rule to replace centroids which haven't been used in a while with a 'fresh' centroid.\n",
        "\n",
        "* (Advanced!) When you have high-dimensional data, you need more centroids to represent the data. When you have lots of centroids, things slow down. What to do?? In *product quantization*, you break a vector into parts and apply k-means to each subvector. Then for any vector, it gets assigned a cluster for each sub-vector. Create a collection of centroids with shape `[K, P, N/P]`, and update your algorithm to train centroids for all of the sub-vectors. Try to stick to Numpy operations as much as possible!"
      ],
      "metadata": {
        "id": "1cY39l-KH3-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fast Data Pipeline with Threading\n",
        "\n",
        "The GIL means that threads are not always helpful in Python, but there are two special cases where Python threading can be helpful:\n",
        "* I/O operations can occur without blocking the GIL, and\n",
        "* Calls to C++/Rust libraries (eg, many librosa operations) release the GIL while doing the heavy lifting.\n",
        "\n",
        "In both of these cases, we can improve execution speed by parallelizing effectively.\n",
        "\n",
        "This is particularly important for *data pipelines*. When training a model, we typically need to feed it subsets of our (gigantic, too big to fit in memory) dataset. Without threading, we need to load the next batch of data, wait for the model to evaluate and update, then load the next batch of data, and so on. This means that our fancy GPU is going to waste while we load the data!\n",
        "\n",
        "Instead, we can load data *while* the model is running, and (hopefully) have the next batch of data ready and waiting. This greatly improves throughput. At the same time, we can make the data loader multi-threaded, to load data from many files in parallel.\n",
        "\n",
        "Let's start with a very simple example of how to use a `ThreadPoolExecutor`."
      ],
      "metadata": {
        "id": "TLghAd3D21Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# The function we want the workers to evaluate.\n",
        "fun = lambda a: a**2\n",
        "\n",
        "# Create an `executor` that we can pass work to.\n",
        "with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "  results = []\n",
        "  for i in range(10):\n",
        "    # Each result is a `future` object, which can tell us whether\n",
        "    # it is still running, and eventually provide a result.\n",
        "    results.append(executor.submit(fun, i))\n",
        "  results = [r.result() for r in results]\n",
        "  print(results)\n",
        "\n",
        "# We can also use a `map` approach more concisely.\n",
        "with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "  # map_results is a generator, yielding results as they are ready.\n",
        "  map_results = executor.map(fun, range(10))\n",
        "  print(list(map_results))\n"
      ],
      "metadata": {
        "id": "QkqPAGZve53s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dataclasses\n",
        "from typing import Generator\n",
        "from etils import epath\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tqdm\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class AudioExample:\n",
        "  audio: np.ndarray\n",
        "  file_id: str\n",
        "  offset: float\n",
        "\n",
        "\n",
        "def data_loader(\n",
        "    base_path: str,\n",
        "    file_glob: str,\n",
        "    window_size_s: float,\n",
        "    sample_rate: int) -> Generator[AudioExample, None, None]:\n",
        "  base_path = epath.Path(base_path)\n",
        "  glob = base_path.glob(file_glob)\n",
        "  window_size = int(window_size_s * sample_rate)\n",
        "  for f in glob:\n",
        "    file_id = f.relative_to(base_path)\n",
        "    audio, _ = librosa.load(f.as_posix(), sr=sample_rate, res_type='polyphase')\n",
        "    if len(audio.shape) == 2:\n",
        "      audio = audio[0]\n",
        "    # pad to multiple of 5s.\n",
        "    pad_samples = window_size - (audio.shape[-1] % window_size)\n",
        "    audio = np.pad(audio, (0, pad_samples))\n",
        "    # reshape into 5s chunks.\n",
        "    audio = np.reshape(audio, [-1, window_size])\n",
        "    for i, a in enumerate(audio):\n",
        "      yield AudioExample(audio=a, file_id=file_id, offset=i * window_size_s)\n",
        "\n",
        "for x in tqdm.tqdm(data_loader(\n",
        "    '/usr/local/google/home/tomdenton/terrorbyte/anuraset',\n",
        "    'raw_data/INCT17/*.wav',\n",
        "    5.0, 32000)):\n",
        "  pass"
      ],
      "metadata": {
        "id": "LF36NLZ4kEiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def data_loader_threaded(\n",
        "    base_path: str,\n",
        "    file_glob: str,\n",
        "    window_size_s: float,\n",
        "    sample_rate: int,\n",
        "    max_workers: int = 2) -> Generator[AudioExample, None, None]:\n",
        "  base_path = epath.Path(base_path)\n",
        "  glob = base_path.glob(file_glob)\n",
        "  window_size = int(window_size_s * sample_rate)\n",
        "\n",
        "  def _pad_reshape(audio):\n",
        "    # pad to multiple of 5s.\n",
        "    pad_samples = window_size - (audio.shape[-1] % window_size)\n",
        "    audio = np.pad(audio, (0, pad_samples))\n",
        "    # reshape into 5s chunks.\n",
        "    audio = np.reshape(audio, [-1, window_size])\n",
        "    return audio\n",
        "\n",
        "  def _load_audio(f: epath.Path) -> np.ndarray:\n",
        "    audio, _ = librosa.load(f.as_posix(), sr=sample_rate, res_type='polyphase')\n",
        "    if len(audio.shape) == 2:\n",
        "      audio = audio[0]\n",
        "    return f, _pad_reshape(audio)\n",
        "\n",
        "  executor = ThreadPoolExecutor(max_workers=max_workers, )\n",
        "  try:\n",
        "    for f, audio in executor.map(_load_audio, glob):\n",
        "      file_id = f.relative_to(base_path)\n",
        "      for i, a in enumerate(audio):\n",
        "        yield AudioExample(audio=a, file_id=file_id, offset=i * window_size_s)\n",
        "  finally:\n",
        "    executor.shutdown(wait=False, cancel_futures=True)\n",
        "\n",
        "for i, x in tqdm.tqdm(enumerate(data_loader_threaded(\n",
        "    '/usr/local/google/home/tomdenton/terrorbyte/anuraset',\n",
        "    'raw_data/INCT17/*.wav',\n",
        "    5.0, 32000, max_workers=2))):\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "Wr1xJCPj9Y2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = epath.Path('/usr/local/google/home/tomdenton/terrorbyte/anuraset')\n",
        "glob = base_path.glob('raw_data/INCT17/*.wav')\n",
        "glob"
      ],
      "metadata": {
        "id": "mhMQvhmhwMEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Challenge: Our current data iterator is pretty good, but has a subtle failure mode. If we process the data slower than the iterator loads the data, we can wind up with an ever-growing backlog of loaded data, until we run out of memory. Refactor the data loader so that it only preloads a fixed number of examples at a time."
      ],
      "metadata": {
        "id": "_zDu_NxSsWGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Itertools\n",
        "\n",
        "In many contexts, we will want to deal with **batches** of examples. Or we might want to manipulate the examples in one way or another.\n",
        "\n",
        "Now that we have a data iterator, we are in good position to make use of `itertools`, a Python package for working with iterators."
      ],
      "metadata": {
        "id": "Ah5vHsuLsAun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro to Jax\n"
      ],
      "metadata": {
        "id": "mhmhC5JTZ9dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use jit to run a matrix multiply on the GPU."
      ],
      "metadata": {
        "id": "zYB1Q7mEviNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiply a vector by a matrix in numpy.\n",
        "\n",
        "input_dim = 128\n",
        "output_dim = 1024\n",
        "\n",
        "inputs = np.random.normal(size=(100, input_dim))\n",
        "weights = np.random.normal(size=(input_dim, output_dim))\n",
        "bias = np.random.normal(size=(output_dim,))\n",
        "\n",
        "%time got = np.dot(inputs, weights) + bias\n",
        "%time got = np.dot(inputs, weights) + bias\n",
        "%time got = np.dot(inputs, weights) + bias"
      ],
      "metadata": {
        "id": "KMBLxJp6rrnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's do it with jax.\n",
        "\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "\n",
        "@jax.jit\n",
        "def dense_layer(inputs, weights, bias):\n",
        "  return jnp.dot(inputs, weights) + bias\n",
        "\n",
        "jinputs = jnp.array(inputs)\n",
        "jweights = jnp.array(weights)\n",
        "jbias = jnp.array(bias)\n",
        "%time got = dense_layer(jinputs, jweights, jbias)\n",
        "%time got = dense_layer(jinputs, jweights, jbias)\n",
        "%time got = dense_layer(jinputs, jweights, jbias)"
      ],
      "metadata": {
        "id": "OVPveWuNOhZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a helper function.\n",
        "\n",
        "def normalize(inputs: jnp.ndarray) -> jnp.ndarray:\n",
        "  return inputs / jnp.linalg.norm(inputs, axis=1, keepdims=True)\n",
        "\n",
        "@jax.jit\n",
        "def dense_layer(inputs, weights, bias):\n",
        "  return jnp.dot(normalize(inputs), weights) + bias\n",
        "\n",
        "jinputs = jnp.array(inputs)\n",
        "jweights = jnp.array(weights)\n",
        "jbias = jnp.array(bias)\n",
        "%time got = dense_layer(jinputs, jweights, jbias)\n",
        "%time got = dense_layer(jinputs, jweights, jbias)\n",
        "%time got = dense_layer(jinputs, jweights, jbias)"
      ],
      "metadata": {
        "id": "iDOVewMJO7_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate data from a mixture of Gaussians using jnp.random."
      ],
      "metadata": {
        "id": "9gt4QdspvZaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: Generate multivariate Gaussian samples using jnp.random.\n",
        "\n",
        "def gaussian_generator(key, num_samples, mu, sigma):\n",
        "  \"\"\"Generate samples from a multivariate Gaussian distribution.\"\"\"\n",
        "  # TODO: Fix this.\n",
        "  while True:\n",
        "    yield jnp.ones((num_samples, mu.shape[0]))\n"
      ],
      "metadata": {
        "id": "7JtVFbzHugZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: Generate a mixture of multivariate Gaussian samples.\n",
        "\n",
        "def mixed_gaussian_generator(key, num_samples, mus, sigmas, pis):\n",
        "  \"\"\"Generate samples from a mixture of Gaussians.\"\"\"\n",
        "  # TODO: Fix this.\n",
        "  while True:\n",
        "    yield jnp.ones((num_samples, mus.shape[0]))\n"
      ],
      "metadata": {
        "id": "5cE-CpQbvAWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution.\n",
        "\n",
        "def mixed_gaussian_generator(key, num_samples, mus, sigmas, pis):\n",
        "  \"\"\"Generate samples from a mixture of Gaussians.\n",
        "\n",
        "  Args:\n",
        "    key: A JAX PRNGKey.\n",
        "    num_samples: The number of samples to generate.\n",
        "    mus: An array of means for the Gaussians, with shape [num_components, dim].\n",
        "    sigmas: Array of standard deviations for the Gaussians.\n",
        "    pis: An array of probabilities for each Gaussian.\n",
        "\n",
        "  Yields:\n",
        "    An array of samples from the mixture of Gaussians.\n",
        "  \"\"\"\n",
        "  num_components, dim = mus.shape\n",
        "  while True:\n",
        "    key = jax.random.split(key)[0]\n",
        "    normal_samples = jax.random.normal(key, shape=(num_samples, dim))\n",
        "    components = jax.random.categorical(\n",
        "        key, jnp.log(jnp.maximum(pis, 1e-6)), shape=(num_samples,))\n",
        "    yield mus[components] + normal_samples * sigmas[components]"
      ],
      "metadata": {
        "id": "fLoLEOXjv_Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "n_components = 3\n",
        "dim = 2\n",
        "mus = jnp.array(np.random.normal(size=(n_components, dim)))\n",
        "sigmas = jnp.array(\n",
        "    np.random.uniform(low=0.1, high=0.4, size=(n_components, dim)))\n",
        "pis = jnp.array(np.random.uniform(size=(n_components,)))\n",
        "pis = pis / jnp.sum(pis)\n",
        "\n",
        "got = next(mixed_gaussian_generator(\n",
        "    key=jax.random.PRNGKey(0),\n",
        "    num_samples=1000,\n",
        "    mus=mus,\n",
        "    sigmas=sigmas,\n",
        "    pis=pis))\n",
        "print(got.shape)\n",
        "plt.scatter(got[:, 0], got[:, 1], alpha=0.25)"
      ],
      "metadata": {
        "id": "K0aOZmrDVEzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer in Jax"
      ],
      "metadata": {
        "id": "UgrOQmVpwCcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: Select data points as the initial centroids.\n",
        "\n",
        "def generate_initial_centroids(\n",
        "    key: jax.random.PRNGKey, data: jnp.ndarray, k: int) -> jnp.ndarray:\n",
        "  pass\n",
        "\n",
        "# Exercise: Assign data points to clusters.\n",
        "\n",
        "def assign_to_clusters(\n",
        "    data: jnp.ndarray, centroids: jnp.ndarray) -> jnp.ndarray:\n",
        "  pass\n",
        "\n",
        "# Exercise: Update the centroids.\n",
        "# This time, instead of replacing the centroid, return an average of the\n",
        "# old centroids and the new centroids, weighted by mu and (1- mu).\n",
        "def update_centroids(\n",
        "    data: jnp.ndarray,\n",
        "    centroids: jnp.ndarray,\n",
        "    mu: float) -> jnp.ndarray:\n",
        "  \"\"\"Update the centroids to be the mean of the data points in each cluster.\"\"\"\n",
        "  pass\n",
        "\n",
        "# Exercise: Compute the reconstruction loss.\n",
        "def reconstruction_loss(data: jnp.ndarray, centroids: jnp.ndarray) -> float:\n",
        "  \"\"\"Compute the mean distance to the closest centroid.\"\"\"\n",
        "  pass\n",
        "\n",
        "# Exercise: Write the training step.\n",
        "@jax.jit\n",
        "def step(\n",
        "    data: jnp.ndarray,\n",
        "    centroids: jnp.ndarray,\n",
        "    mu: float) -> tuple[jnp.ndarray, float]:\n",
        "  \"\"\"Perform one step of k-means clustering.\"\"\"\n",
        "  pass\n",
        "\n"
      ],
      "metadata": {
        "id": "N1XDy16XwHCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training loop.\n",
        "import tqdm\n",
        "pbar = tqdm.tqdm(range(1000))\n",
        "for i in pbar:\n",
        "  data = next(data_generator)\n",
        "  centroids, loss = step(data, centroids, mu)\n",
        "  pbar.set_description(f'loss: {loss:8.3f}')\n"
      ],
      "metadata": {
        "id": "bql9H_y7w8BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer in Jax\n",
        "\n",
        "def generate_initial_centroids(\n",
        "    key: jax.random.PRNGKey, data: jnp.ndarray, k: int) -> jnp.ndarray:\n",
        "  \"\"\"Generate initial centroids for k-means clustering.\"\"\"\n",
        "  return data[jax.random.choice(key, data.shape[0], shape=(k,), replace=False)]\n",
        "\n",
        "\n",
        "def assign_to_clusters(\n",
        "    data: jnp.ndarray, centroids: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Assign data points to clusters.\"\"\"\n",
        "  distances = jnp.linalg.norm(data[:, jnp.newaxis] - centroids, axis=2)\n",
        "  return jnp.argmin(distances, axis=1)\n",
        "\n",
        "\n",
        "def update_centroids(\n",
        "    data: jnp.ndarray,\n",
        "    centroids: jnp.ndarray,\n",
        "    mu: float) -> jnp.ndarray:\n",
        "  \"\"\"Update the centroids to be the mean of the data points in each cluster.\"\"\"\n",
        "  assignments = assign_to_clusters(data, centroids)\n",
        "  for i in range(centroids.shape[0]):\n",
        "    mask = (assignments == i)[:, jnp.newaxis]\n",
        "    data_mu = mu * jnp.sum(data * mask, axis=0) / jnp.maximum(jnp.sum(mask), 1e-6)\n",
        "    updated = (1 - mu) * centroids[i] + data_mu\n",
        "    centroids = centroids.at[i].set(updated)\n",
        "  return centroids\n",
        "\n",
        "def reconstruction_loss(data: jnp.ndarray, centroids: jnp.ndarray) -> float:\n",
        "  \"\"\"Compute the mean distance to the closest centroid.\"\"\"\n",
        "  distances = jnp.linalg.norm(data[:, jnp.newaxis] - centroids, axis=2)\n",
        "  return jnp.mean(jnp.min(distances, axis=1))\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def step(\n",
        "    data: jnp.ndarray,\n",
        "    centroids: jnp.ndarray,\n",
        "    mu: float) -> tuple[jnp.ndarray, float]:\n",
        "  \"\"\"Perform one step of k-means clustering.\"\"\"\n",
        "  centroids = update_centroids(data, centroids, mu)\n",
        "  loss = reconstruction_loss(data, centroids)\n",
        "  return centroids, loss"
      ],
      "metadata": {
        "id": "ZaT4NZL9TM5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a random mixture-of-Gaussians distribution to select data from.\n",
        "seed = np.random.randint(0, 1000000)\n",
        "n_components = 3\n",
        "dim = 2\n",
        "mus = jnp.array(\n",
        "    np.random.normal(size=(n_components, dim)))\n",
        "sigmas = jnp.array(\n",
        "    np.random.uniform(low=0.1, high=0.4, size=(n_components, dim)))\n",
        "pis = jnp.array(\n",
        "    np.random.uniform(size=(n_components,)))\n",
        "pis = pis / jnp.sum(pis)\n",
        "\n",
        "data_generator = mixed_gaussian_generator(\n",
        "    key=jax.random.PRNGKey(seed),\n",
        "    num_samples=1000,\n",
        "    mus=mus,\n",
        "    sigmas=sigmas,\n",
        "    pis=pis)\n",
        "\n",
        "\n",
        "# Train the tokenizer.\n",
        "k = 8\n",
        "mu = 0.1\n",
        "initial_data = next(data_generator)\n",
        "centroids = generate_initial_centroids(\n",
        "    jax.random.PRNGKey(42), initial_data, k)\n",
        "\n",
        "import tqdm\n",
        "pbar = tqdm.tqdm(range(1000))\n",
        "for i in pbar:\n",
        "  data = next(data_generator)\n",
        "  centroids, loss = step(data, centroids, mu)\n",
        "  pbar.set_description(f'loss: {loss:8.3f}')\n"
      ],
      "metadata": {
        "id": "98o4X-kpVVNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Plot some data and the centroids.\n",
        "plt.scatter(initial_data[:, 0], initial_data[:, 1], alpha=0.25)\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], color='red', alpha=0.5)"
      ],
      "metadata": {
        "id": "Z9C6q_rsVoC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Classifier with Optax.\n",
        "\n",
        "Now let's use Optax to write a tiny classifier..."
      ],
      "metadata": {
        "id": "jny2YIVmuveK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some gaussians with labels.\n",
        "\n",
        "def labeled_mixed_gaussian_generator(key, num_samples, mus, sigmas, pis):\n",
        "  \"\"\"Generate samples from a mixture of Gaussians and their labels.\n",
        "\n",
        "  Args:\n",
        "    key: A JAX PRNGKey.\n",
        "    num_samples: The number of samples to generate.\n",
        "    mus: An array of means for the Gaussians, with shape [num_components, dim].\n",
        "    sigmas: Array of standard deviations for the Gaussians.\n",
        "    pis: An array of probabilities for each Gaussian.\n",
        "\n",
        "  Yields:\n",
        "    An array of samples from the mixture of Gaussians and the source components.\n",
        "  \"\"\"\n",
        "  num_components, dim = mus.shape\n",
        "  while True:\n",
        "    key = jax.random.split(key)[0]\n",
        "    normal_samples = jax.random.normal(key, shape=(num_samples, dim))\n",
        "    components = jax.random.categorical(\n",
        "        key, jnp.log(jnp.maximum(pis, 1e-6)), shape=(num_samples,))\n",
        "    features = mus[components] + normal_samples * sigmas[components]\n",
        "    yield features, components\n",
        "\n",
        "num_features = 128\n",
        "batch_size = 64\n",
        "num_classes = 3\n",
        "\n",
        "data_generator = labeled_mixed_gaussian_generator(\n",
        "    key=jax.random.PRNGKey(42),\n",
        "    num_samples=batch_size,\n",
        "    mus=np.random.normal(size=(num_classes, num_features)),\n",
        "    sigmas=np.random.uniform(\n",
        "        low=0.1, high=0.4, size=(num_classes, num_features)),\n",
        "    pis=jnp.ones(num_classes) / num_classes)\n"
      ],
      "metadata": {
        "id": "Hv918f5EWF_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "\n",
        "# Initialize the weights and bias for the linear model.\n",
        "params = {\n",
        "    'weights': jnp.array(np.random.normal(size=(num_features, num_classes))),\n",
        "    'bias': jnp.array(np.random.normal(size=(num_classes,)))\n",
        "}\n",
        "\n",
        "def infer_logits(data, weights, bias):\n",
        "  \"\"\"Infer the labels for the given data.\"\"\"\n",
        "  return jnp.dot(data, weights) + bias\n",
        "\n",
        "optimizer = optax.adam(learning_rate=0.01)\n",
        "opt_state = optimizer.init(params)\n",
        "loss_fn = optax.sigmoid_binary_cross_entropy\n",
        "\n",
        "def compute_loss(params, data, labels):\n",
        "  \"\"\"Compute the loss for the given data and labels.\"\"\"\n",
        "  logits = infer_logits(data, params['weights'], params['bias'])\n",
        "  return jnp.mean(loss_fn(logits, labels))\n",
        "\n",
        "@jax.jit\n",
        "def train_step(data, labels, opt_state, params):\n",
        "  # convert labels to one-hot.\n",
        "  one_hot_labels = jax.nn.one_hot(labels, num_classes)\n",
        "  loss, grad = jax.value_and_grad(compute_loss)(\n",
        "      params, data, one_hot_labels)\n",
        "  updates, opt_state = optimizer.update(grad, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state, loss\n"
      ],
      "metadata": {
        "id": "vw_G9FTWvRHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "pbar = tqdm.tqdm(range(1024))\n",
        "for i in pbar:\n",
        "  data, labels = next(data_generator)\n",
        "  params, opt_state, loss = train_step(data, labels, opt_state, params)\n",
        "  pbar.set_description(f'loss: {loss:8.3f}')\n"
      ],
      "metadata": {
        "id": "QMppvCwYvXaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks with flax\n",
        "\n",
        "Flax provides tools to handle bigger models efficiently.\n",
        "The `flax.nnx` interfaces are used to define model layers, which then join together into the full model. It takes care of all of the parameter naming and such automagically."
      ],
      "metadata": {
        "id": "NMRZjRL3308h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import nnx\n",
        "\n",
        "class TwoLayerDenseModel(nnx.Module):\n",
        "  \"\"\"A simple two-layer neural network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_dim: int,\n",
        "      hidden_dim: int,\n",
        "      output_dim: int,\n",
        "      dropout_rate: float,\n",
        "      rngs: nnx.Rngs):\n",
        "    self.batch_norm = nnx.BatchNorm(num_features=input_dim, rngs=rngs)\n",
        "    self.dense1 = nnx.Linear(input_dim, hidden_dim, rngs=rngs)\n",
        "    self.dropout = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n",
        "    self.dense2 = nnx.Linear(hidden_dim, output_dim, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "    x = self.batch_norm(x)\n",
        "    x = self.dense1(x)\n",
        "    x = jax.nn.relu(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense2(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "pOlxfR5nwfrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.005\n",
        "momentum = 0.9\n",
        "\n",
        "model = TwoLayerDenseModel(\n",
        "    input_dim=num_features,\n",
        "    hidden_dim=32,\n",
        "    output_dim=num_classes,\n",
        "    dropout_rate=0.5,\n",
        "    rngs=nnx.Rngs(0))\n",
        "\n",
        "data, labels = next(data_generator)\n",
        "y = model(data)\n",
        "print(y.shape)\n",
        "\n",
        "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate, momentum))\n",
        "metrics = nnx.MultiMetric(\n",
        "  accuracy=nnx.metrics.Accuracy(),\n",
        "  loss=nnx.metrics.Average('loss'),\n",
        ")\n"
      ],
      "metadata": {
        "id": "eSZj1HAOyEAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(model, features, labels):\n",
        "  logits = model(features)\n",
        "  one_hot_labels = jax.nn.one_hot(labels, num_classes)\n",
        "  loss = optax.sigmoid_binary_cross_entropy(logits, one_hot_labels)\n",
        "  return jnp.mean(loss), logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model, optimizer, metrics, data, labels):\n",
        "  \"\"\"Train one step.\"\"\"\n",
        "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grad = grad_fn(model, data, labels)\n",
        "  metrics.update(loss=loss, logits=logits, labels=labels)\n",
        "  optimizer.update(grad)\n",
        "  return loss\n",
        "\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'train_accuracy': [],\n",
        "}\n",
        "pbar = tqdm.tqdm(range(1024))\n",
        "for step in pbar:\n",
        "  data, labels = next(data_generator)\n",
        "  loss = train_step(model, optimizer, metrics, data, labels)\n",
        "  pbar.set_description(f'loss: {loss:8.3f}')\n",
        "\n",
        "  if step % 64 == 0:\n",
        "    # log the metrics.\n",
        "    for metric, value in metrics.compute().items():\n",
        "      metrics_history[f'train_{metric}'].append(value)\n",
        "    metrics.reset()\n"
      ],
      "metadata": {
        "id": "JtHIjUqPBDoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('step/64')\n",
        "plt.show()\n",
        "plt.plot(metrics_history['train_accuracy'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('step/64')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QvhfsBPEBzaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IA4EXTsUDK9C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
